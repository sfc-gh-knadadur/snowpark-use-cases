{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52708f7c-a42f-4202-8216-d68ae0902e71",
   "metadata": {},
   "source": [
    "### Install mlflow libraries if you have not done it already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3ec80c1-2e9f-4b41-aee0-1404b4d8d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlflow snowflake-snowpark-python==1.0.0 pyarrow==8.0.0 scikit-learn==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeab970-cc71-4fa1-b860-bdd183100711",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snowflake_mlflow-0.0.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b4203-0444-4b19-9f67-d77dcb4241b5",
   "metadata": {},
   "source": [
    "### Import all libraries including snowpark related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451640e5-f89c-4824-9e58-9e88dce6acde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark Imports\n",
    "from snowflake.snowpark.session import Session\n",
    "import snowflake.snowpark.functions as F\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark.functions import sproc, udf, col, log, when, lit\n",
    "# Other Imports\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.deployments import get_deploy_client\n",
    "import json\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "\n",
    "# Import Snowflake Plugin for MLflow\n",
    "from snowflake.ml.mlflow import create_session\n",
    "\n",
    "## from DataBricks\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from scipy.stats.mstats import winsorize   \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27eeef-2cde-403c-b05d-d722cc580670",
   "metadata": {},
   "source": [
    "### Create the snowflake connection with cred.json connection details\n",
    "### Create internal stage in the DB, Schema for storing Models and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf63a91-e513-4cb3-86f9-789ad2a8df13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(status='Stage area MODELS successfully created.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading Snowflake Connection Details\n",
    "snowflake_connection_cfg = json.loads(open('cred.json').read())\n",
    "\n",
    "# Creating Snowpark Session\n",
    "mlflow_poc_session = Session.builder.configs(snowflake_connection_cfg).create()\n",
    "\n",
    "# Create a fresh & new schema\n",
    "mlflow_poc_session.sql('CREATE OR REPLACE SCHEMA MLFLOW_POC_DEMO').collect()\n",
    "mlflow_poc_session.use_schema('MLFLOW_POC_DEMO')\n",
    "\n",
    "# Creating stages for functions, models\n",
    "mlflow_poc_session.sql(\"CREATE STAGE IF NOT EXISTS FUNCTIONS\").collect()\n",
    "mlflow_poc_session.sql(\"CREATE STAGE IF NOT EXISTS MODELS\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69165f92-38fc-4ba4-9432-637ab6604391",
   "metadata": {},
   "source": [
    "### This is a temporary step to load data from csv file to a snowflake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5251350-49ea-4bab-b293-872a484c784a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowflake.snowpark.table.Table at 0x7fdec88a7d30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_CLV_FEATURES_INPUT_df = pd.read_csv('F_CLV_FEATURES_INPUT.csv')\n",
    "mlflow_poc_session.write_pandas(F_CLV_FEATURES_INPUT_df, \n",
    "                                table_name='F_CLV_FEATURES_INPUT', \n",
    "                                auto_create_table=True, \n",
    "                                overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c1a89-a9b6-4028-b909-bb99c69fe233",
   "metadata": {},
   "source": [
    "### Read the snowflake table to snowpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6c5092-f03b-46f2-b4c8-d166fdee65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"F_CLV_FEATURES_INPUT\"\n",
    "\n",
    "# input_data = (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(os.path.join(path, table))).toPandas()\n",
    "input_data = mlflow_poc_session.table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc38e9e-e176-429b-9372-b9953397f1c4",
   "metadata": {},
   "source": [
    "### Print shape of snowpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a877e30-fea9-4f26-93b6-cf151c0bad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000, 95]\n"
     ]
    }
   ],
   "source": [
    "# print(input_data.shape)\n",
    "print([input_data.count(),len(input_data.columns)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361eb50-085e-4707-8860-b00c72bb17c1",
   "metadata": {},
   "source": [
    "### Sample top nsample rows in snowpark dataframe and print shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c53df5-5446-482d-a148-bc50c12d3fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000, 95]\n"
     ]
    }
   ],
   "source": [
    "# nSample = min(14000000, input_data.shape[0])\n",
    "\n",
    "nSample = min(14000000, input_data.count())\n",
    "\n",
    "# data = input_data[:nSample]\n",
    "\n",
    "data = input_data.limit(nSample)\n",
    "\n",
    "# print(input_data.shape)\n",
    "\n",
    "print([data.count(),len(data.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a64e873-cde0-43ea-aa6c-063aaa3be294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to check\n",
    "# del input_data\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e44529-f5fd-4632-add7-7dc96b7cd84b",
   "metadata": {},
   "source": [
    "### define the features (categorical and numeric) and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb70c227-ff7b-44c7-9b61-dc963cfd24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_PROFILE = []\n",
    "\n",
    "VAR_TARGET = ['RETAIL_LTR', 'RETAIL_LTV', 'COM_LTR', 'COM_LTV']\n",
    "\n",
    "FEATURES_CATEGORICAL = ['RTL_RECENCY_12MO', 'RTL_FREQUENCY_12MO', 'RTL_TENURE_12MO', \n",
    "                    'COM_RECENCY_12MO', 'COM_FREQUENCY_12MO', 'COM_TENURE_12MO',\n",
    "                    'VERTICAL', 'EMP_SIZE',  'IS_TEACHER', 'IS_PARENT',]\n",
    "\n",
    "FEATURES_NUMERIC = [\n",
    "       'RETAIL_NET_SALES', \n",
    "       'RETAIL_NET_SALES_SUPPLIES', \n",
    "       'RETAIL_NET_SALES_COMPUTERS',\n",
    "       'RETAIL_NET_SALES_FACILITIES', 'RETAIL_NET_SALES_FOOD',\n",
    "       'RETAIL_NET_SALES_FURNITURES', 'RETAIL_NET_SALES_INK',\n",
    "       'RETAIL_NET_SALES_MAIL', 'RETAIL_NET_SALES_SERVICES',\n",
    "       'RETAIL_NET_SALES_PRINTERS', 'RETAIL_NET_SALES_PAPER',\n",
    "       'RETAIL_NET_SALES_PRINT', 'RETAIL_NET_SALES_WARRANTIES',\n",
    "       'RETAIL_NET_SALES_TECH_ACCESSORIES', 'RETAIL_NET_SALES_OTHER',\n",
    "            \n",
    "       'COM_NET_SALES',\n",
    "       'COM_NET_SALES_SUPPLIES', 'COM_NET_SALES_COMPUTERS',\n",
    "       'COM_NET_SALES_FACILITIES', 'COM_NET_SALES_FOOD',\n",
    "       'COM_NET_SALES_FURNITURES', 'COM_NET_SALES_INK', 'COM_NET_SALES_MAIL',\n",
    "       'COM_NET_SALES_SERVICES', 'COM_NET_SALES_PRINTERS',\n",
    "       'COM_NET_SALES_PAPER', 'COM_NET_SALES_PRINT',\n",
    "       'COM_NET_SALES_WARRANTIES', 'COM_NET_SALES_TECH_ACCESSORIES',\n",
    "       'COM_NET_SALES_OTHER', \n",
    "    \n",
    "       'RETAIL_MARGIN', \n",
    "       'RETAIL_MARGIN_SUPPLIES', 'RETAIL_MARGIN_COMPUTERS',\n",
    "       'RETAIL_MARGIN_FACILITIES', 'RETAIL_MARGIN_FOOD',\n",
    "       'RETAIL_MARGIN_FURNITURES', 'RETAIL_MARGIN_INK', \n",
    "       'RETAIL_MARGIN_MAIL',\n",
    "       'RETAIL_MARGIN_SERVICES', 'RETAIL_MARGIN_PRINTERS',\n",
    "       'RETAIL_MARGIN_PAPER', 'RETAIL_MARGIN_PRINT',\n",
    "       'RETAIL_MARGIN_WARRANTIES', 'RETAIL_MARGIN_TECH_ACCESSORIES',\n",
    "       'RETAIL_MARGIN_OTHER', \n",
    "    \n",
    "       'COM_MARGIN', 'COM_MARGIN_SUPPLIES',\n",
    "       'COM_MARGIN_COMPUTERS', 'COM_MARGIN_FACILITIES', 'COM_MARGIN_FOOD',\n",
    "       'COM_MARGIN_FURNITURES', 'COM_MARGIN_INK', 'COM_MARGIN_MAIL',\n",
    "       'COM_MARGIN_SERVICES', 'COM_MARGIN_PRINTERS', 'COM_MARGIN_PAPER',\n",
    "       'COM_MARGIN_PRINT', 'COM_MARGIN_WARRANTIES',\n",
    "       'COM_MARGIN_TECH_ACCESSORIES', 'COM_MARGIN_OTHER', \n",
    "\n",
    "        'RETAIL_SFC', 'RETAIL_VFC',\n",
    "        'COM_SFC', 'COM_VFC',\n",
    "        \n",
    "       'RETAIL_NET_SALES_RETURNS', 'RETAIL_MARGIN_RETURNS',\n",
    "       'COM_NET_SALES_RETURNS', 'COM_MARGIN_RETURNS',\n",
    "    \n",
    "       'RETAIL_NET_SALES_KIOSK', 'RETAIL_MARGIN_KIOSK', \n",
    "       'COM_NET_SALES_BOPIS', 'COM_MARGIN_BOPIS', \n",
    "       \n",
    "       'RETAIL_SRWSTM', 'RETAIL_SRWINK', \n",
    "       'COM_SRWSTM', 'COM_SRWINK',\n",
    "       'RETAIL_NET_SALES_STAPLES_BRAND', 'RETAIL_MARGIN_STAPLES_BRAND',\n",
    "       'COM_NET_SALES_STAPLES_BRAND', 'COM_MARGIN_STAPLES_BRAND']\n",
    "\n",
    "VAR_SUBSET = FEATURES_CATEGORICAL + FEATURES_NUMERIC + VAR_TARGET\n",
    "\n",
    "VAR_TOTAL = VAR_SUBSET + VAR_PROFILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f99e9e-1bca-4325-85d1-a8c8137fc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code Comment start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4861d545-ef3a-4117-9986-07e20331bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func_numeric_log(df):\n",
    "#     for clsName in FEATURES_NUMERIC:\n",
    "#         ###change to snowpark\n",
    "#         df[clsName] = np.where(df[clsName]<0, -np.log(1-df[clsName]), np.log(1+df[clsName]))\n",
    "#     return df\n",
    "# def func_feature_preproc(df):\n",
    "    \n",
    "#     df_log = func_numeric_log(df=df)\n",
    "    \n",
    "#     output = df_log[VAR_TOTAL]\n",
    "    \n",
    "#     return output\n",
    "# df_preproc = func_feature_preproc(df=data.to_pandas())\n",
    "\n",
    "# # print(df_preproc.shape)\n",
    "\n",
    "# # del data\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b419da5-53b0-4092-890d-88e7bca650d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func_numeric_log(df):\n",
    "#     for clsName in FEATURES_NUMERIC:\n",
    "#         df[clsName] = np.where(df[clsName]<0, -np.log(1-df[clsName]), np.log(1+df[clsName]))\n",
    "#     return df\n",
    "\n",
    "# def func_feature_preproc(df):\n",
    "    \n",
    "#     df_log = func_numeric_log(df=df)\n",
    "    \n",
    "#     output = df_log[VAR_TOTAL]\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac55ba5-ee93-405d-952c-9e0b08660ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code Comment end here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5a311-e881-4a4c-af0e-c5114f58befb",
   "metadata": {},
   "source": [
    "### Define same function with snowpark dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40968899-00bf-4b8c-985a-fcec6ca54109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_numeric_log(df):\n",
    "    for clsName in FEATURES_NUMERIC:\n",
    "        ### snowpark dataframe function\n",
    "        df=df.with_column(clsName,when(col(clsName)>=0, F.log(2.71828,1+data.col(clsName))) \\\n",
    "         .when(col(clsName)<0, -F.log(2.71828,1-data.col(clsName))) \\\n",
    "         .otherwise(lit(0)))\n",
    "\n",
    "    return df\n",
    "\n",
    "def func_feature_preproc(df):\n",
    "    df_log = func_numeric_log(df=df)    \n",
    "    output = df_log.select(VAR_TOTAL)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da7117-fc5c-4956-9681-5b0b7c482507",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c4c8b45-788f-49f4-80de-93bac47ee13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_preproc = func_feature_preproc(df=data.to_pandas())\n",
    "\n",
    "# print(df_preproc.shape)\n",
    "\n",
    "# ### need to find replacement in snowpark\n",
    "# # del data\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961b8909-7bf4-4333-bcac-10069545bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code end here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7ad3f-a045-4bb5-8349-df033e8b8572",
   "metadata": {},
   "source": [
    "### Call snowpark dataframe embedded in the function on the data and store output as transient table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee66f57-e512-4496-8aee-0232020b466b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000, 94]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "create_temp_table is deprecated. We still respect this parameter when it is True but please consider using `table_type=\"temporary\"` instead.\n"
     ]
    }
   ],
   "source": [
    "df_preproc = func_feature_preproc(df=data)\n",
    "print([df_preproc.count(),len(df_preproc.columns)])\n",
    "mlflow_poc_session.create_dataframe(df_preproc.to_pandas())\\\n",
    "                  .write.mode(\"overwrite\")\\\n",
    "                  .save_as_table(\"staple_df_preproc\",table_type=\"transient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc62245-25b0-4244-9fa4-7ff6a7771baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Created this below piece of code to register separate winsorize stored proc but as I explained I have build a consolidated Stored Proc all the way below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b362c900-9b69-40e6-ba0b-13da3cf75b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func_winsorize_train(df_train_input, features):\n",
    "#     df_train_winsorize = df_train_input.copy(deep=True)\n",
    "\n",
    "#     dic_winsorize_min = {} \n",
    "#     dic_winsorize_max = {} \n",
    "\n",
    "#     for clsName in features:\n",
    "#         df_train_winsorize[clsName] = winsorize(df_train_input[clsName], limits=[0.01, 0.01]).data\n",
    "\n",
    "#         dic_winsorize_min[clsName] = df_train_winsorize[clsName].min()\n",
    "#         dic_winsorize_max[clsName] = df_train_winsorize[clsName].max()\n",
    "    \n",
    "#     return df_train_winsorize, dic_winsorize_min, dic_winsorize_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355b6ba5-f9a1-430b-85b7-d10aaf9ab0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @sproc(name='func_winsorize_train',\n",
    "#        packages=['snowflake-snowpark-python','pandas','scipy'],\n",
    "#        stage_location='@MODELS',\n",
    "#        is_permanent=True,\n",
    "#        replace=True)\n",
    "# def func_winsorize_train(session: Session, inp_table_name: str ,\n",
    "#                          out_table_name: str, \n",
    "#                          features:list)->T.Variant:\n",
    "#     from scipy.stats.mstats import winsorize   \n",
    "#     import pandas as pd\n",
    "#     df_train_winsorize = session.table(inp_table_name).select(*features).to_pandas().copy(deep=True)\n",
    "\n",
    "#     dic_winsorize_min = {} \n",
    "#     dic_winsorize_max = {} \n",
    "\n",
    "#     for clsName in features:\n",
    "#         df_train_winsorize[clsName] = winsorize(df_train_winsorize[clsName], limits=[0.01, 0.01]).data\n",
    "        \n",
    "#         dic_winsorize_min[clsName] = df_train_winsorize[clsName].min()\n",
    "#         dic_winsorize_max[clsName] = df_train_winsorize[clsName].max()\n",
    "        \n",
    "#     session.create_dataframe(df_train_winsorize)\\\n",
    "#     .write.mode(\"overwrite\")\\\n",
    "#     .save_as_table(out_table_name,table_type=\"transient\")\n",
    "\n",
    "#     return {'winsorize_min':dic_winsorize_min,'winsorize_max':dic_winsorize_max}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a14289-2a36-4a74-a5eb-dc17597bc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d4f30fc-43ce-4359-aafa-e1b6ba5b55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMMAND ----------\n",
    "\n",
    "# X_winsorize, winsorize_feature_numeric_min, winsorize_feature_numeric_max = func_winsorize_train(df_train_input=df_preproc, features=FEATURES_NUMERIC)\n",
    "# y_winsorize, winsorize_target_min, winsorize_target_max = func_winsorize_train(df_train_input=df_preproc, features=VAR_TARGET)\n",
    "\n",
    "# #--\n",
    "# print(X_winsorize.shape)\n",
    "# print(y_winsorize.shape)\n",
    "\n",
    "# # del df_preproc\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "147de146-958b-45b4-968b-0b69f1ad3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMMAND ----------\n",
    "# winsorize_feature_numeric = func_winsorize_train(\"staple_df_preproc\",\n",
    "#                                                  \"staple_df_features_winsorize\",\n",
    "#                                                  FEATURES_NUMERIC)\n",
    "\n",
    "# winsorize_target = func_winsorize_train(\"staple_df_preproc\",\n",
    "#                                         \"staple_df_target_winsorize\",\n",
    "#                                         VAR_TARGET)\n",
    "\n",
    "# X_winsorize = mlflow_poc_session.table(\"staple_df_features_winsorize\")\n",
    "# Y_winsorize = mlflow_poc_session.table(\"staple_df_target_winsorize\")\n",
    "\n",
    "# winsorize_feature_numeric_min = eval(winsorize_feature_numeric)['winsorize_min']\n",
    "# winsorize_feature_numeric_max = eval(winsorize_feature_numeric)['winsorize_max']\n",
    "\n",
    "# winsorize_target_min = eval(winsorize_target)['winsorize_min']\n",
    "# winsorize_target_max = eval(winsorize_target)['winsorize_max']\n",
    "\n",
    "# #--\n",
    "# print([X_winsorize.count(),len(X_winsorize.columns)])\n",
    "# print([Y_winsorize.count(),len(Y_winsorize.columns)])\n",
    "\n",
    "# # need to find equivalent in snowpark\n",
    "# # del df_preproc\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "611de835-b3e8-466c-b123-773a2e47f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks code end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d77408-f4a5-4cf8-b7d3-fc030caa07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Created this below piece of code to register separate standard stored proc but as I explained I have build a consolidated Stored Proc all the way below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe27e24d-bcb3-485c-aac3-ffe524420bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @sproc(name='func_standard_scale_train',\n",
    "#        packages=['snowflake-snowpark-python','pandas','scikit-learn==1.1.1'],\n",
    "#        stage_location='@MODELS',\n",
    "#        is_permanent=True,\n",
    "#        replace=True)\n",
    "# def func_standard_scale_train(session: Session, \n",
    "#                               inp_table_name: str ,\n",
    "#                              out_table_name: str, \n",
    "#                              features:list)->str:\n",
    "#     from sklearn import preprocessing   \n",
    "#     import pandas as pd\n",
    "#     X_numeric = session.table(inp_table_name).select(*features).to_pandas().copy(deep=True)\n",
    "    \n",
    "#     scaler_feature_numeric = preprocessing.StandardScaler()\n",
    "#     scaler_feature_numeric = scaler_feature_numeric.fit(X_numeric)\n",
    "        \n",
    "#     # session.create_dataframe(scaler_feature_numeric)\\\n",
    "#     # .write.mode(\"overwrite\")\\\n",
    "#     # .save_as_table(out_table_name,table_type=\"transient\")\n",
    "\n",
    "#     return scaler_feature_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b765cb0a-e180-453b-bb6d-cb121eb39a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func_standard_scale_train(\"STAPLE_DF_PREPROC_FEATURES_WINSORIZE\",\n",
    "#                                                  \"staple_df_features_std_scale\",\n",
    "#                                                  FEATURES_NUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6fe51-deed-4722-ae4c-7f652e126db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f61a38a-1b1c-4ca6-b164-9232bd78cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del df_preproc\n",
    "# gc.collect()\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# X_numeric = X_winsorize[FEATURES_NUMERIC]\n",
    "# scaler_feature_numeric = preprocessing.StandardScaler()\n",
    "# scaler_feature_numeric = scaler_feature_numeric.fit(X_numeric)\n",
    "\n",
    "# #--\n",
    "# print(X_numeric.shape)\n",
    "\n",
    "# del X_numeric, X_winsorize, y_winsorize\n",
    "# gc.collect()\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # MAGIC %md #SAVE parameters for winsorize and scaler\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# import mlflow\n",
    "# import mlflow.sklearn\n",
    "\n",
    "# mlflow.set_tracking_uri(\"databricks\")\n",
    "# mlflow.set_experiment(\"/Users/adminzhali001@ussicorp5.onmicrosoft.com/LTV_train_score_prd/LTV_Experiment/LTV_TRAIN_DE_14M\")\n",
    "\n",
    "# with mlflow.start_run(nested=True) as run:\n",
    "#     run_id = run.info.run_id\n",
    "#     print(run_id)\n",
    "    \n",
    "#     mlflow.sklearn.log_model(scaler_feature_numeric,  \"scaler_feature_numeric\")\n",
    "#     mlflow.sklearn.log_model(winsorize_feature_numeric_min,  \"winsorize_feature_numeric_min\")\n",
    "#     mlflow.sklearn.log_model(winsorize_feature_numeric_max,  \"winsorize_feature_numeric_max\")\n",
    "#     mlflow.sklearn.log_model(winsorize_target_min,  \"winsorize_target_min\")\n",
    "#     mlflow.sklearn.log_model(winsorize_target_max,  \"winsorize_target_max\")\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # MAGIC %md #SAVE RUN_ID TO DATBRICKS\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# run_id_hist =  (spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"/FileStore/tables/run_id\")).toPandas()\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# df_runId = pd.DataFrame({'name':'winsorize_scale',\n",
    "#                          'run_day':datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "#                          'run_id': run_id}, index=[0])\n",
    "\n",
    "# df_runId_full = run_id_hist.append(df_runId)\n",
    "\n",
    "# df_runId_full = df_runId_full.assign(name = lambda f: f['name'].astype(str))\\\n",
    "#             .assign(run_day = lambda f: f['run_day'].astype(str))\\\n",
    "#             .assign(run_id = lambda f: f['run_id'].astype(str)) \\\n",
    "#             .reset_index(drop=True)\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# spark_run_id = spark.createDataFrame(df_runId_full)\n",
    "# spark_run_id.write.mode('overwrite').option(\"header\",True).csv(\"/FileStore/tables/run_id\")\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # MAGIC %md # CLEANING CACHE\n",
    "\n",
    "# # COMMAND ----------\n",
    "\n",
    "# # MAGIC %reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78568c7-619a-40ca-b01d-fb2b8d272189",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Databricks Code end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aec8a-ddb0-4743-9b5e-8d4fdbc0e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete Stored Proc for winsorize (feature numeric, target) and Standard Scale the numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c02db3-7887-4649-9c06-4b72f7d90c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The version of package joblib in the local environment is 1.2.0, which does not fit the criteria for the requirement joblib. Your UDF might not work when the package version is different between the server and your local environment\n",
      "The version of package mlflow in the local environment is 2.1.1, which does not fit the criteria for the requirement mlflow. Your UDF might not work when the package version is different between the server and your local environment\n"
     ]
    }
   ],
   "source": [
    "### Wrap by defining the stored proc by calling the packages required , model storage, function that will be registered.\n",
    "@sproc(name='func_winsorize_std_scale',\n",
    "       packages=['snowflake-snowpark-python','pandas','scipy','scikit-learn==1.1.1','joblib','mlflow'],\n",
    "       stage_location='@MODELS',\n",
    "       is_permanent=True,\n",
    "       replace=True)\n",
    "### Define the function by calling the input parameters with the corresponding datatypes and the output as variant\n",
    "def func_winsorize_std_scale(session: Session, \n",
    "                             inp_table_name: str,\n",
    "                             features:list,\n",
    "                             targets:list,\n",
    "                             stage_name: str,\n",
    "                             experiment_name: str)->T.Variant:\n",
    "    ### Import the required libraries as usual\n",
    "    import mlflow\n",
    "    import io\n",
    "    import joblib\n",
    "    # set MLflow-path to /tmp\n",
    "    mlflow.set_tracking_uri('/tmp/mlruns')\n",
    "    mlflow.sklearn.autolog()\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "    \n",
    "    # start the mlflow experiment\n",
    "    run = mlflow.start_run()\n",
    "\n",
    "    from scipy.stats.mstats import winsorize   \n",
    "    import pandas as pd\n",
    "    \n",
    "    # load the table and convert to pandas in a dataframe for features numeric\n",
    "    df_features_winsorize = session.table(inp_table_name).select(*features).to_pandas().copy(deep=True)\n",
    "    \n",
    "    dic_winsorize_features_min = {} \n",
    "    dic_winsorize_features_max = {} \n",
    "    \n",
    "    # Apply the winsorize function on every variable and update the dataframe\n",
    "    for clsName in features:\n",
    "        df_features_winsorize[clsName] = winsorize(df_features_winsorize[clsName], limits=[0.01, 0.01]).data\n",
    "\n",
    "        # Get the min and max value per feature\n",
    "        dic_winsorize_features_min[clsName] = df_features_winsorize[clsName].min()\n",
    "        dic_winsorize_features_max[clsName] = df_features_winsorize[clsName].max()\n",
    "\n",
    "    # optionally Store the winsorized features in a transient table    \n",
    "    out_table_name=inp_table_name+\"_features\"+\"_winsorize\"        \n",
    "    session.create_dataframe(df_features_winsorize)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .save_as_table(out_table_name,table_type=\"transient\")\n",
    "\n",
    "    # load the table and convert to pandas in a dataframe for the targets    \n",
    "    df_targets_winsorize = session.table(inp_table_name).select(*targets).to_pandas().copy(deep=True)\n",
    "\n",
    "    dic_winsorize_targets_min = {} \n",
    "    dic_winsorize_targets_max = {} \n",
    "\n",
    "    for clsName in targets:\n",
    "        df_targets_winsorize[clsName] = winsorize(df_targets_winsorize[clsName], limits=[0.01, 0.01]).data\n",
    "\n",
    "        # Get the min and max value per target\n",
    "        dic_winsorize_targets_min[clsName] = df_targets_winsorize[clsName].min()\n",
    "        dic_winsorize_targets_max[clsName] = df_targets_winsorize[clsName].max()\n",
    "\n",
    "    # optionally Store the winsorized targets in a transient table       \n",
    "    out_table_name=inp_table_name+\"_targets\"+\"_winsorize\"        \n",
    "    session.create_dataframe(df_targets_winsorize)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .save_as_table(out_table_name,table_type=\"transient\")\n",
    "\n",
    "    from sklearn import preprocessing   \n",
    "    import pandas as pd\n",
    "\n",
    "    # Define the Standard scaler and fit the winsorized features dataframe\n",
    "    scaler_feature_numeric = preprocessing.StandardScaler()\n",
    "    scaler_feature_numeric = scaler_feature_numeric.fit(df_features_winsorize)\n",
    "\n",
    "    # Log the model information through mlflow\n",
    "    mlflow.sklearn.log_model(scaler_feature_numeric,  \"scaler_feature_numeric\")\n",
    "    mlflow.sklearn.log_model(dic_winsorize_features_min,  \"winsorize_feature_numeric_min\")\n",
    "    mlflow.sklearn.log_model(dic_winsorize_features_max,  \"winsorize_feature_numeric_max\")\n",
    "    mlflow.sklearn.log_model(dic_winsorize_targets_min,  \"winsorize_target_min\")\n",
    "    mlflow.sklearn.log_model(dic_winsorize_targets_max,  \"winsorize_target_max\")\n",
    "    mlflow.end_run()\n",
    "    \n",
    "    # Get the run id information in a variable\n",
    "    run = mlflow.get_run(run.info.run_id)\n",
    "    \n",
    "    # Save model to Snowflake stage\n",
    "    input_stream = io.BytesIO()\n",
    "    joblib.dump(scaler_feature_numeric, input_stream)\n",
    "    model_path = f'@{stage_name}/mlflow_models/{experiment_name}.joblib'\n",
    "    session._conn._cursor.upload_stream(input_stream, model_path)\n",
    "    \n",
    "    # Return mlflow tracking and model path\n",
    "    experiment_run = run.to_dictionary()\n",
    "    experiment_run['SNOWFLAKE_MODEL_PATH'] = model_path\n",
    "    \n",
    "    # Capture the run id information in a pandas dataframe\n",
    "    df_runId = pd.DataFrame({'name':'winsorize_scale',\n",
    "                         'run_day':datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                         'run_id': str(experiment_run)}, index=[0])\n",
    "    # define the log table\n",
    "    log_table_name = \"MLFLOW_LTV_STAPLES_LOG\"\n",
    "    \n",
    "    # read the history log table in pandas dataframe and append the new run id\n",
    "    run_id_hist = session.table(log_table_name).to_pandas()\n",
    "    df_runId_full = run_id_hist.append(df_runId)\n",
    "    \n",
    "    # append the dataframe to the full pandas dataframe\n",
    "    df_runId_full = df_runId_full.assign(name = lambda f: f['name'].astype(str))\\\n",
    "                .assign(run_day = lambda f: f['run_day'].astype(str))\\\n",
    "                .assign(run_id = lambda f: f['run_id'].astype(str)) \\\n",
    "                .reset_index(drop=True)\n",
    "\n",
    "    #Write the dataframe to the table\n",
    "    session.create_dataframe(df_runId_full)\\\n",
    "    .write.mode(\"overwrite\")\\\n",
    "    .save_as_table(log_table_name)\n",
    "    \n",
    "    # return the experiment as variant\n",
    "    return experiment_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4204a-114c-43ed-b890-3a85da346708",
   "metadata": {},
   "source": [
    "### Create mlflow log table if you have not created already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd5c7dff-d7e7-4141-8b77-b1fdca33bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_poc_session.sql('create or replace table MLFLOW_LTV_STAPLES_LOG (\"name\" string,\"run_day\" timestamp, \"run_id\" variant)').collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9833eb2-7cf4-49bb-ba85-7ed1fda49374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input table as string to read\n",
    "inp_table_name='staple_df_preproc'\n",
    "# stage location as string for model to save\n",
    "stage_name='MODELS'\n",
    "# numeric features as list\n",
    "features=FEATURES_NUMERIC\n",
    "# experiment name as string\n",
    "experiment_name=\"LTV_STAPLES\",\n",
    "# targets list as list\n",
    "targets=VAR_TARGET\n",
    "# registered model name\n",
    "registered_model_name = 'LTV_STAPLES_MODEL'\n",
    "\n",
    "# just incase an mlflow run is alreayd in progress\n",
    "mlflow.end_run()\n",
    "\n",
    "# start mlflow run\n",
    "run = mlflow.start_run()\n",
    "# Train the model, returns mlflow run as dict\n",
    "mlflow_dict = json.loads(func_winsorize_std_scale('staple_df_preproc', \n",
    "                                                  FEATURES_NUMERIC, \n",
    "                                                  VAR_TARGET,\n",
    "                                                  'MODELS',\n",
    "                                                  'LTV_STAPLES'))\n",
    "mlflow_dict\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7de20159-9a96-4793-9ea9-73b5c6c90b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "|\"name\"           |\"run_day\"            |\"run_id\"                                            |\n",
      "----------------------------------------------------------------------------------------------\n",
      "|winsorize_scale  |17/02/2023 14:25:48  |{'info': {'artifact_uri': '/tmp/mlruns/44181700...  |\n",
      "----------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show=['run_id']\n",
    "mlflow_poc_session.sql(\"SELECT * FROM MLFLOW_LTV_STAPLES_LOG\").show()\n",
    "# check the table where the experiment is captured as variant.\n",
    "# for best experiment login to snowflake worksheet to see the output on the variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4f0b4-6c08-47d8-a717-4265b84dae60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
